{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "# importing stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading tokenizer for english language\n",
    "spacy_nlp_eng = spacy.load('en_core_web_sm')\n",
    "spacy_nlp_esp = spacy.load('es_core_news_md')\n",
    "# create list of punctuations and stopwords\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "spanish_stopwords = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence, lang): \n",
    "    # remove distracting single quotes\n",
    "    sentence = re.sub('\\'','',sentence)\n",
    "    # remove digits and words containing digits\n",
    "    sentence = re.sub('\\w*\\d\\w*','',sentence)\n",
    "    # replace extra spaces with single space\n",
    "    sentence = re.sub(' +',' ',sentence)\n",
    "    # remove unwanted lines starting from special characters\n",
    "    sentence = re.sub(r'\\n: \\'\\'.*','',sentence)\n",
    "    sentence = re.sub(r'\\n!.*','',sentence)\n",
    "    sentence = re.sub(r'^:\\'\\'.*','',sentence)    \n",
    "    # remove non-breaking new line characters\n",
    "    sentence = re.sub(r'\\n',' ',sentence)    \n",
    "    # remove punctuations\n",
    "    sentence = re.sub(r'[^\\w\\s]',' ',sentence)\n",
    "    \n",
    "    # creating token object\n",
    "    if lang == \"eng\":\n",
    "        tokens = spacy_nlp_eng(sentence)\n",
    "    elif lang == \"esp\":\n",
    "        tokens = spacy_nlp_esp(sentence)\n",
    "    # lower, strip and lemmatize\n",
    "    tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens]    \n",
    "    # remove stopwords, and exclude words less than 2 characters\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_eng = \"The track features lead singer Lauren Mayberry recalling horrific statements made to her by men — He said, You need to be fed, but keep an eye on your waistline — over looming synthesizers. The track was made in quarantine, while Mayberry and Martin Doherty were in Los Angeles and Iain Cook was in Glasgow, Scotland.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The track features lead singer Lauren Mayberry recalling horrific statements made to her by men — He said, You need to be fed, but keep an eye on your waistline — over looming synthesizers. The track was made in quarantine, while Mayberry and Martin Doherty were in Los Angeles and Iain Cook was in Glasgow, Scotland.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_esp = \"El trío de pop electrónico de Glasgow formado por Lauren Mayberry, Lan Cook y Martin Doherty estrena su flamante nuevo single He Said, She Said. Con este reivindicativo himno synth pop, CHVRCHES regresan a los ávidos oídos de sus fans, retomando su actividad tras casi dos años de silencio.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El trío de pop electrónico de Glasgow formado por Lauren Mayberry, Lan Cook y Martin Doherty estrena su flamante nuevo single He Said, She Said. Con este reivindicativo himno synth pop, CHVRCHES regresan a los ávidos oídos de sus fans, retomando su actividad tras casi dos años de silencio.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['track',\n",
       " 'feature',\n",
       " 'lead',\n",
       " 'singer',\n",
       " 'lauren',\n",
       " 'mayberry',\n",
       " 'recall',\n",
       " 'horrific',\n",
       " 'statement',\n",
       " 'man',\n",
       " 'need',\n",
       " 'feed',\n",
       " 'eye',\n",
       " 'waistline',\n",
       " 'loom',\n",
       " 'synthesizer',\n",
       " 'track',\n",
       " 'quarantine',\n",
       " 'mayberry',\n",
       " 'martin',\n",
       " 'doherty',\n",
       " 'los',\n",
       " 'angeles',\n",
       " 'iain',\n",
       " 'cook',\n",
       " 'glasgow',\n",
       " 'scotland']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_eng = tokenizer(text_eng, lang = \"eng\")\n",
    "tokens_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['triar',\n",
       " 'pop',\n",
       " 'electrónico',\n",
       " 'glasgow',\n",
       " 'formar',\n",
       " 'por',\n",
       " 'lauren',\n",
       " 'mayberry',\n",
       " 'lan',\n",
       " 'cook',\n",
       " 'martin',\n",
       " 'doherty',\n",
       " 'estrenar',\n",
       " 'flamante',\n",
       " 'nuevo',\n",
       " 'singlar',\n",
       " 'said',\n",
       " 'said',\n",
       " 'con',\n",
       " 'este',\n",
       " 'reivindicativo',\n",
       " 'himno',\n",
       " 'synth',\n",
       " 'pop',\n",
       " 'chvrches',\n",
       " 'regresar',\n",
       " 'ávido',\n",
       " 'oír',\n",
       " 'fan',\n",
       " 'retomar',\n",
       " 'actividad',\n",
       " 'tras',\n",
       " 'casi',\n",
       " 'año',\n",
       " 'silenciar']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_esp = tokenizer(text_esp, lang = \"esp\")\n",
    "tokens_esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus(tokens, save_dict = False):\n",
    "    # create dictionary\n",
    "    dictionary = corpora.Dictionary()\n",
    "    # corpus creation \n",
    "    corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in tokens]\n",
    "    # calcalating words frequency\n",
    "    word_frequency = [[(dictionary[id], count) for id, count in doc] for doc in corpus]\n",
    "\n",
    "    # saving dictionary and corpus\n",
    "    if save_dict == True:\n",
    "        dictionary.save('dictionary.dict')  \n",
    "        corpora.MmCorpus.serialize('corpus.mm', corpus)\n",
    "\n",
    "    return dictionary, corpus, word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_eng, corpus_eng, word_frequency_eng = generate_corpus([tokens_eng])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_esp, corpus_esp, word_frequency_esp = generate_corpus([tokens_esp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.similarities import MatrixSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(dictionary, corpus):\n",
    "    # load tfidf model\n",
    "    tfidf_model = gensim.models.TfidfModel(corpus, id2word=dictionary)\n",
    "    # load lsi model and extract corpus to be used in matrixsimilarity function\n",
    "    lsi_model = gensim.models.LsiModel(tfidf_model[corpus], id2word=dictionary, num_topics=300)\n",
    "    gensim.corpora.MmCorpus.serialize('lsi_model.mm',lsi_model[tfidf_model[corpus]])  \n",
    "    lsi_corpus = gensim.corpora.MmCorpus('lsi_model.mm')\n",
    "    # similarity indexes\n",
    "    index = MatrixSimilarity(lsi_corpus, num_features = lsi_corpus.num_terms)\n",
    "\n",
    "    return tfidf_model, lsi_model, index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_words(search_term, dictionary, tfidf_model, lsi_model, index):\n",
    "    # transform search terms to tokens and create the corpus\n",
    "    query_bow = [dictionary.doc2bow(cleansing.tokenizer(search_term))]\n",
    "    # apply tfidf model to the corpus\n",
    "    query_tfidf = tfidf_model[query_bow]\n",
    "    # apply lsi model to the tfidf\n",
    "    query_lsi = lsi_model[query_tfidf]\n",
    "    # number of elements with the maximum relevance\n",
    "    index.num_best = 5\n",
    "    \n",
    "    return index[query_lsi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\luise.barranco\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\lsimodel.py:102: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rel_spectrum = np.abs(1.0 - np.cumsum(s / np.sum(s)))\n",
      "c:\\users\\luise.barranco\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\lsimodel.py:104: RuntimeWarning: invalid value encountered in greater\n",
      "  small = 1 + len(np.where(rel_spectrum > min(discard, 1.0 / k))[0])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot index a corpus with zero features (you must specify either `num_features` or a non-empty corpus in the constructor)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b2a3e2a28476>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfidf_model_eng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsi_model_eng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_eng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary_eng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_eng\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-686619815964>\u001b[0m in \u001b[0;36mcreate_models\u001b[1;34m(dictionary, corpus)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mlsi_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMmCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lsi_model.mm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# similarity indexes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMatrixSimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlsi_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlsi_corpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtfidf_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsi_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\luise.barranco\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\gensim\\similarities\\docsim.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_best, dtype, num_features, chunksize, corpus_len)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_features\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m                 raise ValueError(\n\u001b[1;32m--> 790\u001b[1;33m                     \u001b[1;34m\"cannot index a corpus with zero features (you must specify either `num_features` \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    791\u001b[0m                     \u001b[1;34m\"or a non-empty corpus in the constructor)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: cannot index a corpus with zero features (you must specify either `num_features` or a non-empty corpus in the constructor)"
     ]
    }
   ],
   "source": [
    "tfidf_model_eng, lsi_model_eng, index_eng = create_models(dictionary_eng, corpus_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
